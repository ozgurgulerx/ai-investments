{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Path to your CSV files (assuming they are in the same directory)\n",
    "file_path_pattern = \"2411*.csv\"\n",
    "\n",
    "# Initialize total line count\n",
    "total_lines = 0\n",
    "\n",
    "# Count lines in each file and print them\n",
    "for file in glob.glob(file_path_pattern):\n",
    "    with open(file, 'r') as f:\n",
    "        line_count = sum(1 for line in f) - 1  # Exclude header row\n",
    "        print(f\"{file}: {line_count} lines\")\n",
    "        total_lines += line_count\n",
    "\n",
    "# Read and concatenate all CSV files\n",
    "all_files = glob.glob(file_path_pattern)\n",
    "df = pd.concat([pd.read_csv(file) for file in all_files], ignore_index=True)\n",
    "\n",
    "# Check if merged DataFrame has the correct total lines (including header)\n",
    "merged_line_count = len(df)\n",
    "print(f\"Merged file line count (excluding header): {merged_line_count}\")\n",
    "print(\"Line count verification:\", \"PASS\" if merged_line_count == total_lines else \"FAIL\")\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file if needed\n",
    "df.to_csv(\"merged_investments.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'investments1.csv' with the path to one of your files\n",
    "df_sample = pd.read_csv('merged_investments.csv')\n",
    "print(df_sample.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Path to your CSV files\n",
    "file_path_pattern = \"merged_investments.csv\"\n",
    "\n",
    "# Columns to drop\n",
    "columns_to_drop = [\n",
    "     'Industries'\n",
    "]\n",
    "\n",
    "# Read, drop columns, and concatenate all CSV files\n",
    "all_files = glob.glob(file_path_pattern)\n",
    "df = pd.concat([pd.read_csv(file).drop(columns=columns_to_drop) for file in all_files], ignore_index=True)\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file if needed\n",
    "df.to_csv(\"merged_investments.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Chroma db \n",
    "chroma run &\n",
    "\n",
    "#sqlite3 \n",
    "sqlite3 ai_investments.db \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#understand the table structure to create the db in sqlite3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the merged CSV file to inspect its structure and columns\n",
    "# Assuming the file is named 'merged_investments.csv'\n",
    "file_path = \"merged_investments.csv\"\n",
    "\n",
    "try:\n",
    "    # Load a sample of the merged CSV file to examine its structure\n",
    "    df_merged = pd.read_csv(file_path)\n",
    "    # Display the first few rows and column names to understand the data\n",
    "    df_merged_head = df_merged.head()\n",
    "    df_merged_columns = df_merged.columns\n",
    "    df_merged_info = df_merged.info()\n",
    "except FileNotFoundError:\n",
    "    df_merged_head = \"File not found.\"\n",
    "    df_merged_columns = \"File not found.\"\n",
    "    df_merged_info = \"File not found.\"\n",
    "\n",
    "df_merged_head, df_merged_columns, df_merged_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the sqllite3 table \n",
    "\n",
    "CREATE TABLE investments (\n",
    "    \"Organization Name\" TEXT,\n",
    "    \"Last Funding Type\" TEXT,\n",
    "    \"Last Funding Date\" TEXT,\n",
    "    \"Last Funding Amount (in USD)\" INTEGER,\n",
    "    \"Headquarters Location\" TEXT,\n",
    "    \"Description\" TEXT,\n",
    "    \"Top 5 Investors\" TEXT,\n",
    "    \"Founders\" TEXT,\n",
    "    \"Most Recent Valuation Range\" TEXT,\n",
    "    \"Founded Date\" TEXT,\n",
    "    \"Twitter\" TEXT,\n",
    "    \"LinkedIn\" TEXT,\n",
    "    \"Full Description\" TEXT,\n",
    "    \"Website\" TEXT\n",
    ");\n",
    "\n",
    "\n",
    "sqlite> .tables\n",
    "investments\n",
    "sqlite> .schema investments\n",
    "CREATE TABLE investments (\n",
    "    \"Organization Name\" TEXT,\n",
    "    \"Last Funding Type\" TEXT,\n",
    "    \"Last Funding Date\" TEXT,\n",
    "    \"Last Funding Amount (in USD)\" INTEGER,\n",
    "    \"Headquarters Location\" TEXT,\n",
    "    \"Description\" TEXT,\n",
    "    \"Top 5 Investors\" TEXT,\n",
    "    \"Founders\" TEXT,\n",
    "    \"Most Recent Valuation Range\" TEXT,\n",
    "    \"Founded Date\" TEXT,\n",
    "    \"Twitter\" TEXT,\n",
    "    \"LinkedIn\" TEXT,\n",
    "    \"Full Description\" TEXT,\n",
    "    \"Website\" TEXT\n",
    ");\n",
    "sqlite>\n",
    "\n",
    "\n",
    "#now install the .csv into the table \n",
    ".mode csv\n",
    ".import merged_investments.csv investments\n",
    "\n",
    "sqlite> .mode csv\n",
    "sqlite> .import merged_investments.csv investments\n",
    "sqlite> SELECT * FROM investments LIMIT 5;\n",
    ".exit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First check if there is any db in chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb import Client\n",
    "\n",
    "# Initialize the ChromaDB client\n",
    "client = Client()\n",
    "\n",
    "# List all existing collections\n",
    "collections = client.list_collections()\n",
    "\n",
    "# Check if there are any collections\n",
    "if collections:\n",
    "    print(\"Existing collections:\")\n",
    "    for collection in collections:\n",
    "        print(f\"- Collection name: {collection.name}, Number of items: {collection.count()}\")\n",
    "else:\n",
    "    print(\"No collections found in ChromaDB.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install embeddings to chromadb along with organization name metadata\n",
    "out of these columns in the data we will only keep the \"Organisation Name\" as it is and take the embeddings of \"Description\" and \"Full Description\" and add them to chroma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first check if there are any collections in the chromadb\n",
    "\n",
    "from chromadb import Client\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "client = Client()\n",
    "\n",
    "# List all existing collections\n",
    "collections = client.list_collections()\n",
    "\n",
    "# Check if there are any collections and display their details\n",
    "if collections:\n",
    "    print(\"Existing collections in ChromaDB:\")\n",
    "    for collection in collections:\n",
    "        collection_name = collection.name\n",
    "        item_count = collection.count()  # Count the number of items in the collection\n",
    "        print(f\"- Collection name: {collection_name}, Number of items: {item_count}\")\n",
    "else:\n",
    "    print(\"No collections found in ChromaDB. Database is empty.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "from openai import AzureOpenAI\n",
    "import chromadb\n",
    "\n",
    "# Set up logging for every 100 embeddings processed\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Suppress detailed logs from the openai library\n",
    "logging.getLogger(\"openai\").setLevel(logging.ERROR)\n",
    "\n",
    "# Initialize AzureOpenAI client\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "    api_version=\"2024-06-01\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_API_ENDPOINT\")\n",
    ")\n",
    "\n",
    "# Function to get embeddings with a check for NaN\n",
    "def get_embedding(text, model=\"text-embedding-3-small-eastus\"):\n",
    "    if isinstance(text, float):  # Check if the text is NaN\n",
    "        return None\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "\n",
    "# Load the merged CSV file\n",
    "df = pd.read_csv(\"merged_investments.csv\")\n",
    "\n",
    "# Initialize columns for embeddings\n",
    "df['description_embedding'] = None\n",
    "df['full_description_embedding'] = None\n",
    "\n",
    "# Generate embeddings for both \"Description\" and \"Full Description\", with logging every 100 rows\n",
    "for i in range(len(df)):\n",
    "    # Generate embeddings for \"Description\" and \"Full Description\" for each row\n",
    "    df.at[i, 'description_embedding'] = get_embedding(df.at[i, 'Description'])\n",
    "    df.at[i, 'full_description_embedding'] = get_embedding(df.at[i, 'Full Description'])\n",
    "\n",
    "    # Log progress every 100 rows\n",
    "    if (i + 1) % 100 == 0:\n",
    "        logging.info(f\"Processed {i + 1} embeddings out of {len(df)} rows\")\n",
    "\n",
    "# Drop rows where either embedding is missing\n",
    "df = df.dropna(subset=['description_embedding', 'full_description_embedding'])\n",
    "\n",
    "# Combine the embeddings (e.g., concatenate or average; here we concatenate)\n",
    "df['combined_embedding'] = [desc + full_desc for desc, full_desc in zip(df['description_embedding'], df['full_description_embedding'])]\n",
    "\n",
    "# Initialize ChromaDB client and create collection\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.create_collection(\"ai_investments\")\n",
    "\n",
    "# Add records to ChromaDB\n",
    "collection.add(\n",
    "    documents=df['Description'].tolist(),  # Use descriptions as documents\n",
    "    embeddings=df['combined_embedding'].tolist(),  # Store combined embeddings\n",
    "    metadatas=[{'Organization Name': name} for name in df['Organization Name']],  # Only \"Organization Name\" as metadata\n",
    "    ids=[f\"id_{i}\" for i in range(len(df))]  # Unique ids for each entry\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ok next, lets'check if the embeddings are now stored in the chromadb and make a query to it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch sample data from the collection without using embeddings\n",
    "sample_results = collection.get(\n",
    "    limit=5  # Adjust the number as needed to retrieve a sample of items\n",
    ")\n",
    "\n",
    "# Display sample items\n",
    "for i, metadata in enumerate(sample_results['metadatas']):\n",
    "    print(f\"Item {i + 1}:\")\n",
    "    print(\"Organization Name:\", metadata.get(\"Organization Name\"))\n",
    "    print(\"Document:\", sample_results['documents'][i])\n",
    "    print(\"\\n---\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ok now let's do a similarity query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from openai import AzureOpenAI\n",
    "from chromadb import Client\n",
    "\n",
    "# Set up logging for minimal output\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logging.getLogger(\"openai\").setLevel(logging.ERROR)\n",
    "\n",
    "# Initialize AzureOpenAI client for generating embeddings\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "    api_version=\"2024-06-01\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_API_ENDPOINT\")\n",
    ")\n",
    "\n",
    "# Function to generate embeddings for text queries\n",
    "def get_embedding(text, model=\"text-embedding-3-small-eastus\"):\n",
    "    if isinstance(text, float):  # Check if the text is NaN\n",
    "        return None\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "\n",
    "# Initialize the ChromaDB client and get the collection\n",
    "chroma_client = Client()\n",
    "collection = chroma_client.get_collection(\"ai_investments\")  # Ensure this matches your collection name\n",
    "\n",
    "# Function to search for similar startups using query embeddings\n",
    "def search_similar_startups(query_text, top_n=5):\n",
    "    # Generate two embeddings for the query text and concatenate them to match the 3072-dimensional format\n",
    "    description_embedding = get_embedding(query_text)\n",
    "    full_description_embedding = get_embedding(query_text)\n",
    "    \n",
    "    if description_embedding is None or full_description_embedding is None:\n",
    "        print(\"Failed to generate embeddings for the query text.\")\n",
    "        return []\n",
    "    \n",
    "    # Concatenate the two embeddings\n",
    "    query_embedding = description_embedding + full_description_embedding  # Now 3072 dimensions\n",
    "    \n",
    "    # Query the ChromaDB collection with the concatenated embedding\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_n,\n",
    "        include=[\"metadatas\", \"documents\", \"distances\"]  # Retrieve metadata, documents, and distances\n",
    "    )\n",
    "    \n",
    "    # Flatten metadata and distances if necessary\n",
    "    flattened_metadatas = [item for sublist in results['metadatas'] for item in sublist]\n",
    "    distances = results.get('distances', [])\n",
    "    flattened_distances = [distance for sublist in distances for distance in sublist] if isinstance(distances[0], list) else distances\n",
    "    \n",
    "    # Extract startup names and similarity scores\n",
    "    startup_results = [\n",
    "        {\"startup_name\": result.get(\"Organization Name\"), \"similarity_score\": 1 - distance}  # Convert distance to similarity score\n",
    "        for result, distance in zip(flattened_metadatas, flattened_distances)\n",
    "        if \"Organization Name\" in result\n",
    "    ]\n",
    "    \n",
    "    return startup_results\n",
    "\n",
    "\n",
    "# Example usage: Search for similar startups based on a query\n",
    "query = \"ecommerce\"\n",
    "similar_startups = search_similar_startups(query, top_n=100)\n",
    "\n",
    "# Print the results with similarity scores\n",
    "for startup in similar_startups:\n",
    "    print(f\"Startup: {startup['startup_name']}, Similarity Score: {startup['similarity_score']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query sqlite3 e.g. \n",
    "sqlite> SELECT * FROM investments WHERE \"Organization Name\" = 'PropulsionAI';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import csv\n",
    "import logging\n",
    "from openai import AzureOpenAI\n",
    "from chromadb import Client\n",
    "\n",
    "# Set up logging for minimal output\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logging.getLogger(\"openai\").setLevel(logging.ERROR)\n",
    "\n",
    "# Initialize AzureOpenAI client for generating embeddings\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "    api_version=\"2024-06-01\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_API_ENDPOINT\")\n",
    ")\n",
    "\n",
    "# Function to generate embeddings for text queries\n",
    "def get_embedding(text, model=\"text-embedding-3-small-eastus\"):\n",
    "    if isinstance(text, float):  # Check if the text is NaN\n",
    "        return None\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "\n",
    "# Initialize the ChromaDB client and get the collection\n",
    "chroma_client = Client()\n",
    "collection = chroma_client.get_collection(\"ai_investments\")  # Ensure this matches your collection name\n",
    "\n",
    "# Function to search for similar startups using concatenated query embeddings\n",
    "def search_similar_startups(query_text, top_n=100):\n",
    "    # Generate two embeddings for the query text and concatenate them to match the 3072-dimensional format\n",
    "    description_embedding = get_embedding(query_text)\n",
    "    full_description_embedding = get_embedding(query_text)\n",
    "    \n",
    "    if description_embedding is None or full_description_embedding is None:\n",
    "        print(\"Failed to generate embeddings for the query text.\")\n",
    "        return []\n",
    "    \n",
    "    # Concatenate the two embeddings to form a 3072-dimensional vector\n",
    "    query_embedding = description_embedding + full_description_embedding  # Now 3072 dimensions\n",
    "    \n",
    "    # Query the ChromaDB collection with the concatenated embedding\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_n,\n",
    "        include=[\"metadatas\", \"documents\", \"distances\"]  # Retrieve metadata, documents, and distances\n",
    "    )\n",
    "    \n",
    "    # Flatten metadata and distances if necessary\n",
    "    flattened_metadatas = [item for sublist in results['metadatas'] for item in sublist]\n",
    "    distances = results.get('distances', [])\n",
    "    flattened_distances = [distance for sublist in distances for distance in sublist] if isinstance(distances[0], list) else distances\n",
    "    \n",
    "    # Extract startup names and similarity scores\n",
    "    startup_results = [\n",
    "        {\"startup_name\": result.get(\"Organization Name\"), \"similarity_score\": 1 - distance}  # Convert distance to similarity score\n",
    "        for result, distance in zip(flattened_metadatas, flattened_distances)\n",
    "        if \"Organization Name\" in result\n",
    "    ]\n",
    "    \n",
    "    # Sort results from most similar to least similar\n",
    "    startup_results.sort(key=lambda x: x[\"similarity_score\"], reverse=True)\n",
    "    \n",
    "    return startup_results\n",
    "\n",
    "# Function to retrieve all fields from SQLite for a list of startup names\n",
    "def fetch_startup_details_from_sqlite(startup_names):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect('ai_investments.db')\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Prepare a list to store the startup details\n",
    "    all_startup_details = []\n",
    "    \n",
    "    # Query for each startup name\n",
    "    for name in startup_names:\n",
    "        query = \"SELECT * FROM investments WHERE `Organization Name` = ?\"\n",
    "        cursor.execute(query, (name,))\n",
    "        rows = cursor.fetchall()\n",
    "        \n",
    "        # If results are found, add them to the list\n",
    "        for row in rows:\n",
    "            all_startup_details.append(row)\n",
    "    \n",
    "    # Close the database connection\n",
    "    conn.close()\n",
    "    \n",
    "    return all_startup_details\n",
    "\n",
    "# Main function to search, retrieve details, and save as CSV\n",
    "def search_and_export_to_csv(query_text, top_n=100, output_dir=\"/Users/ozgurguler/Downloads\"):\n",
    "    # Search for similar startups\n",
    "    similar_startups = search_similar_startups(query_text, top_n=top_n)\n",
    "    \n",
    "    # Extract just the names from the search results\n",
    "    startup_names = [startup[\"startup_name\"] for startup in similar_startups]\n",
    "    \n",
    "    # Fetch details from SQLite for the similar startups\n",
    "    startup_details = fetch_startup_details_from_sqlite(startup_names)\n",
    "    \n",
    "    # Define column names (adjust according to your database schema)\n",
    "    column_names = [\"Organization Name\", \"Last Funding Type\", \"Last Funding Date\", \"Last Funding Amount (in USD)\",\n",
    "                    \"Headquarters Location\", \"Description\", \"Top 5 Investors\", \"Founders\", \"Most Recent Valuation Range\",\n",
    "                    \"Founded Date\", \"Twitter\", \"LinkedIn\", \"Full Description\", \"Website\"]\n",
    "    \n",
    "    # Create the output file path based on the query text\n",
    "    output_file = os.path.join(output_dir, f\"{query_text}_startups.csv\")\n",
    "    \n",
    "    # Save the results to a CSV file\n",
    "    with open(output_file, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(column_names)  # Write header\n",
    "        writer.writerows(startup_details)  # Write rows\n",
    "    \n",
    "    print(f\"Results saved to {output_file}\")\n",
    "\n",
    "# Example usage: Search for a similar startup based on a query and export results\n",
    "query = \"retail e-commerce\"\n",
    "search_and_export_to_csv(query, top_n=100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
